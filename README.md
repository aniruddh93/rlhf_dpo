# RLHF & DPO algorithms for Hopper environment 

* Implements Reinforcement Learning through Human feedback (RLHF) algorithm for the [Hopper](https://gymnasium.farama.org/environments/mujoco/hopper/) environment using dataset of preferred moves to learn a reward model.

* Implements Direct Preference Optimization (DPO) algorithm for the Hopper environment for comparison with the RLHF algorithm. 
